#!/usr/bin/python
#
#
# cmd_standby copyright command prompt inc
#
#
# $Id$
"""
License

Copyright Command Prompt, Inc.

Permission to use, copy, modify, and distribute this software and its
documentation for any purpose, without fee, and without a written agreement
is hereby granted, provided that the above copyright notice and this
paragraph and the following two paragraphs appear in all copies.

IN NO EVENT SHALL COMMAND PROMPT, INC BE LIABLE TO ANY PARTY FOR
DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES, INCLUDING
LOST PROFITS, ARISING OUT OF THE USE OF THIS SOFTWARE AND ITS DOCUMENTATION,
EVEN IF COMMAND PROMPT, INC HAS BEEN ADVISED OF THE POSSIBILITY OF
SUCH DAMAGE.

COMMAND PROMPT, INC SPECIFICALLY DISCLAIMS ANY WARRANTIES,
INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND
FITNESS FOR A PARTICULAR PURPOSE. THE SOFTWARE PROVIDED HEREUNDER IS ON AN
"AS IS" BASIS, AND COMMAND PROMPT, INC HAS NO OBLIGATIONS TO
PROVIDE MAINTENANCE, SUPPORT, UPDATES, ENHANCEMENTS, OR MODIFICATIONS.
"""

import os
import shutil
import sys
import re
import subprocess

from sys import *
from os import system
from cmd_worker import CMDWorker
from time import sleep


argslist = (("-A", "--action", dict(dest="pgctl_action", action="store", help="Start/Stop PostgreSQL", metavar="start|stop|stop_basebackup")),
            ("-B", "--basebackup", dict(dest="base_backup", action="store_true", help="Start a base backup", metavar="FILE")),
            ("-C", "--config", dict(dest="configfilename", action="store", help="Name of the archiver config file", metavar="FILE", default='cmd_standby.ini')),
            ("-F", "--failover", dict(dest="failover", action="store", help="Value is a 3 digit number 999", metavar="VALUE")),
            ("-I", "--dbinit", dict(dest="dbinit", action="store_true", help="Use before -B", metavar="FILE")),
            ("-P", "--ping", dict(dest="ping_check", action="store_true", help="Is my master alive?", metavar="FILE")),
            ("-R", "--recovertotime", dict(dest="recovertotime", action="store", help="To restore to a specific point in time", metavar="TIMESTAMP")),
            ("-S", "--standby", dict(dest="standby", action="store_true", help="Enter standby mode", metavar="FILE")))

classdict = (('pgversion', 's', None),
             ('rsync_flags', 's', ""),
             ('rsync_threads', 'i', ""),
             ('master_public_ip', 's', None),
             ('master_local_ip', 's', ""),
             ('user', 's', None),
             ('debug', 'b', False),
             ('ssh_debug', 'b', False),
             ('port', 'i', None),
             ('ssh_timeout', 'i', None),
             ('archivedir', 's', None),
             ('includepath', 's', None),
             ('pgdata', 's', None),
             ('pg_conf', 's', ""),
             ('pg_hba_conf', 's', ""),
             ('pg_conf_failover', 's', ""),
             ('pg_hba_conf_failover', 's', ""),
             ('no_copy_conf', 'b', False),
             ('recovery_conf', 's', ""),
             ('logfile', 's', ""),
             ('action_failover', 's', ""),
             ('use_streaming_replication', 'b', False),
             ('trigger_file', 's', ""),
             ('repl_db_user', 's', 'postgres'),
             ('repl_db_password', 's', ""),
             ('sslmode', 's', 'prefer'),
             ('notify_ok', 's', None),
             ('notify_warning', 's', None),
             ('notify_critical', 's', None))


class RsyncFailure(Exception):
    """ Class to propagate rsync exceptions"""
    pass


class SSHFailure(Exception):
    """ Class to propagate SSH errors, i.e. wrong password """
    pass


class RemoteNoData(Exception):
    """ Class to propagate errors when remote host returns empty data """
    pass


class CMDStandby(CMDWorker):

    # Validate the command line
    # XXX: replace those parser.error with exceptions in the future
    # for now, they seem to be fine, cause they only happen in the very
    # beginning, when to resources are allocated yet.
    @staticmethod
    def command_line_check_func(parser, options):
        if options.configfilename == None:
            parser.error("option -C is required")

        if options.recovertotime and not (options.failover == '999'):
            parser.error("option -R requires open -F999")

        if options.pgctl_action:
            valid_action = ['start', 'stop', 'stop_basebackup', 'start_basebackup']
            if options.pgctl_action not in valid_action:
                parser.error("option -A requires one of start, stop, stop_basebackup or start_basebackup")

    def get_remote_psql(self):
        """Get path to psql on master"""
        if 'master_public_ip' in vars(self):
            if 'includepath' not in vars(self):
                raise Exception("CONFIG: No path in config file, can't find executables")
            path = " ".join(self.includepath.split(os.pathsep))
            ssh = '%s -o PasswordAuthentication=no %s' % (self.ssh, self.master_public_ip)
            getpaths = subprocess.check_output('%s "find %s -ignore_readdir_race -name psql"' % (ssh, path), shell=True)
            getpaths = getpaths.strip().split("\n")
            if getpaths:
                self.r_psql = getpaths[0]
            else:
                raise Exception("Couldn't find remote psql")

    def get_bin_paths_func(self, options):
        exes = ["pg_ctl"]
        if not self.use_streaming_replication:
            exes.append("pg_standby")
        else:
            if options.recovertotime:
                raise Exception('CONFIG: Unable to use recovery_target_time with streaming replication')
            exes.append("pg_archivecleanup")

        super(CMDStandby, self).get_bin_paths_func(options, CMDWorker.COMMON_BIN_NAMES + exes)

    # Let's make sure archive directory and config files can be reached
    def check_config_func(self, options):
        if int(self.pgversion[0]) < 9:
            raise Exception('CONFIG: PITRTools only supports PostgreSQL versions 9.0+')

        for element in (self.pg_conf, self.pg_hba_conf):
            os.stat(element)  # raises OSError on fail

        # make a new directory if archivedir doesn't exist.
        if not os.access(self.archivedir, os.R_OK | os.W_OK | os.X_OK):
            os.makedirs(self.archivedir, 0700)

        if options.failover:
            # check that the configuration files are all set.
            if not self.pg_conf_failover or not self.pg_hba_conf_failover:
                raise Exception("CONFIG: failover is requested but pg_conf_failover and/or pg_hba_conf_failover are not set, exiting")
            else:
                for f in (self.pg_conf_failover, self.pg_hba_conf_failover):
                    if not os.access(f, os.F_OK | os.R_OK):
                        raise Exception("CONFIG: unable to read the failover file %s", str(f))

    # Create pg_xlog dir if missing, follow symlink if present
    # XXX: change os.stat to os.access
    def check_pgxlog_path_func(self):
        pg_xlog_dir = "%s/%s" % (str(self.pgdata), str('pg_xlog'))
        success = False
        try:
            pg_xlog_dir = os.path.realpath(pg_xlog_dir)
            # XXX: we might be able to stat the directory, but still get the
            # undersired permissions (i.e no write access).
            os.stat(pg_xlog_dir)
            success = True
        except:
            try:
                self.debuglog("check_pgxlog_path_func(): pg_xlog_dir = %s" % pg_xlog_dir)
                os.makedirs(pg_xlog_dir, 0700)
            except OSError, e:
                self.log("check_pgxlog_path_func(): %s" % e, "ERROR")
                self.log("You may have permission problems; Make sure user %s can create directory %s" % (self.user, pg_xlog_dir))
            except Exception, e:
                self.log("check_pgxlog_path_func(): %s" % e, "ERROR")
            else:
                success = True
        return success

    def set_rsync_options(self):
        self.rsync_flags = "-al %s --delete " % (self.rsync_flags)
        if self.debug:
            self.rsync_flags += '-v --stats '

    def set_pg_standby_options(self):
        self.pg_standby_flags = '-s5 -w0 -c '
        if self.debug:
            self.pg_standby_flags += '-d '
        self.pg_standby_args = "%f %p %r "

    def set_connect_and_copy_options(self, options):
        # Yes the odd counted " is needed because of the way we have to quote within the command
        # There may be a better way to do this, but I got tired of fighting.
        ssh_connect = """%s %s %s@%s """ % (self.ssh, self.ssh_flags, self.user, self.master_public_ip)
        psql_connect = """ "%s -A -t -U%s -p%s -dpostgres """ % (self.r_psql, self.user, self.port)

        if self.master_local_ip:
            psql_connect += '-h%s ' % (self.master_local_ip,)
        self.copy_dirs = [self.rsync]
        self.copy_dirs.extend(self.rsync_flags.split())
        self.copy_dirs.extend(["--exclude=pg_log/", "--exclude=pg_xlog/", "--exclude=postgresql.conf", "--exclude=pg_hba.conf", "--exclude=postmaster.pid", "-e",
                               self.ssh, "%s@%s:" % (self.user, self.master_public_ip)])
        self.ssh_psql = ssh_connect + psql_connect
        self.pgctl_base = [self.pg_ctl, "-D", self.pgdata]
        if self.no_copy_conf:
            # we don't copy configuration files to the data directory, instead,
            # set command-line options to get them from their pristine locations
            if not options.failover:
                pg_conf = self.pg_conf
                pg_hba_conf = self.pg_hba_conf
            else:
                pg_conf = self.pg_conf_failover
                pg_hba_conf = self.pg_hba_conf_failover
            # XXX: pg_ctl is sensitive to the order of -c options: 
            # config_file always go first, additional '-o' options doesn't work,
            # so both postgresql.conf and pg_hba.conf should be in a single -o.
            config_file_opts = ""
            if pg_conf and os.access(pg_conf, os.F_OK | os.R_OK):
                config_file_opts += "-c config_file=%s " % (pg_conf,)
            if pg_hba_conf and os.access(pg_hba_conf, os.F_OK | os.R_OK):
                config_file_opts += "-c hba_file=%s " % (pg_hba_conf,)
            if config_file_opts:
                self.pgctl_base += ["-o",config_file_opts]
        if self.logfile:
            self.pgctl_base += ["-l",self.logfile]

    def set_recovery_options(self, options):
        # set more sane value for trigger and recovery files now that we know
        # pgdata value:
        if self.trigger_file == "":
            self.trigger_file = "%s/cmd_end_recovery" % (self.pgdata,)
        if self.recovery_conf == "":
            self.recovery_conf = """%s/recovery.conf""" % (self.pgdata,)
        # Recovery string for recovery.conf
        self.recovery_stopped_file = re.sub(r'recovery.conf$', 'recovery.stopped', self.recovery_conf)

        # 3 different recovery_string options:
        # no streaming replication and no failover - use pg_standby with restore_command
        # SR - include both primary_conninfo and restore_command w/o pg_standby.
        # failover - use only resore_command w/o pg_standby.

        if self.use_streaming_replication and options.failover != '9999':
            restore_command = """'cp %s/%%f "%%p"' """ % (self.archivedir,)
            cleanup_string = """'%s %s %%r'""" % (self.pg_archivecleanup, self.archivedir)
            primary_conninfo_string = "host=%s port=%s user=%s sslmode=%s " % (self.master_public_ip, self.port, self.repl_db_user, self.sslmode)
            # streaming replication and not failover, if password is not supplied
            # it's expected to be found in .pgpass or via PGPASSWORD variable.
            # see http://www.postgresql.org/docs/current/static/libpq-envars.html
            if len(self.repl_db_password) > 0:
                primary_conninfo_string += "password=%s " % (self.repl_db_password,)
            self.recovery_string = """standby_mode = 'on'\nprimary_conninfo = '%s'\ntrigger_file = '%s'\nrestore_command = %s\narchive_cleanup_command = %s """ % (primary_conninfo_string, self.trigger_file, restore_command, cleanup_string)
        elif options.failover == '999':
            self.recovery_string = """restore_command = 'cp %s/%%f "%%p"' """ % (self.archivedir,)
            if options.recovertotime:
                self.recovery_string += """\nrecovery_target_time = '%s' """ % (options.recovertotime,)
        else:
            self.recovery_string = """restore_command = '%s %s %s %s' """ % (self.pg_standby, self.pg_standby_flags, self.archivedir, self.pg_standby_args)

    def set_options(self, options):
        """ Wrapper for varions set_ functions """
        self.set_ssh_flags()
        self.set_rsync_options()
        self.set_pg_standby_options()
        self.set_connect_and_copy_options(options)
        self.set_recovery_options(options)

    # Check the master for being alive
    def ping_check_func(self):
        success = False
        try:
            output = self.exec_query_on_primary("""'SELECT 1'""", False)
        except RemoteNoData, e:
            self.log("ping_check_func(): remote host returned no data for query: %s" % e, "ERROR")
        except SSHFailure:
            self.log("ping_check_func(): Received error code 255 from ssh, likely due to the wrong password", "ERROR")
        except Exception, e:
            self.log("ping_check_func(): %s" % e, "ERROR")
        else:
            for row in output:
                row = row.rstrip('\n')
                self.debuglog("ping_check_func(): row = %s" % row)
                if str(row) != "1":
                    self.notify_external(critical=True, message="no response from master")
                else:
                    success = True
        if success:
            print "Got response from the master"
        else:
            print "ERROR: no response from the master"
        return success

    # TODO: replace popen with subprocess module calls.
    def exec_query_on_primary(self, query, emptyok=True):
        """ Runs a database query on the primary node and returns the psql output"""
        self.debuglog("exec_query_on_primarY: executing query `%s' via: %s" % (query, self.ssh_psql))
        p = os.popen("%s -c %s\"" % (self.ssh_psql, query))
        result = p.readlines()
        exitstatus = p.close()
        if exitstatus:
            # check whether ssh has terminated due to an error occurred, likely an incorrect password
            if (exitstatus>>8) == 255:
                raise SSHFailure
            raise Exception("command returned non-zero exit status")
        if len(result) == 0 and not emptyok:
            raise RemoteNoData(query)
        return result

    # This function gives us all the non pgdata directories required
    # for operation, such as table spaces
    def primary_get_tablespace_paths(self):
        try:
            paths = self.exec_query_on_primary("""'SELECT * FROM cmd_get_data_dirs()'""")
        except:
            self.log("primary_get_tablespace_paths(): Unable to get namespace paths; did you apply the helper scripts in cmd_standby.sql?", "ERROR")
            raise
        return paths

    def primary_get_datadir_path(self):
        try:
            path = self.exec_query_on_primary("""'SELECT * FROM cmd_get_pgdata() LIMIT 1'""", False)
        except RemoteNoData:
            self.log("primary_get_datadir_path(): Unable to get namespace paths; did you apply the helper scripts in cmd_standby.sql?", "ERROR")
            raise
        return path[0]

    # Start a base backup on the master
    # First we issue a checkpoint and then a start backup
    def start_backup_func(self):
        retval = os.system("rm -rf %s/*" % (self.archivedir,))
        if retval:
            self.log("Unable to rm old archives: return code %d" % retval, "WARNING")
        output = self.exec_query_on_primary(""" 'checkpoint' """, False)
        for row in output:
            row = row.rstrip('\n')
            self.debuglog("start_backup_func(): row = %s" % row)
            if str(row) != "CHECKPOINT":
                self.notify_external(log=True, critical=True, message="Unable to execute CHECKPOINT")
                raise Exception("ERROR: Unable to execute CHECKPOINT")
        output = self.exec_query_on_primary(""" 'SELECT cmd_pg_start_backup()' """, False)
        for row in output:
            row = row.rstrip('\n')
            self.debuglog("start_backup_func(): cmd_pg_start_backup: row = %s" % row)
            if str(row) != "1":
                raise Exception("ERROR: Unable to start base backup")

    def stop_backup_func(self):
        """
        Stop base backup. This function catches all incoming exception inside and
        indicates errors with return status, unlike other _backup counterparts. The
        rationale is that since it's the last call in a function we can avoid an
        extra try..catch block.
        """
        success = False
        try:
            output = self.exec_query_on_primary(""" 'SELECT cmd_pg_stop_backup()' """)
        except RemoteNoData, e:
            self.log("stop_backup_func(): remote host returned no data for query: %s" % e, "ERROR")
        except SSHFailure:
            self.log("stop_backup_func(): received error code 255 from ssh, likely due to the wrong password", "ERROR")
        except Exception, e:
            self.log("stop_backup_func(): %s" % e, "ERROR")
        else:
            for row in output:
                row = row.rstrip('\n')
                self.debuglog("stop_backup_func(): cmd_pg_stop_backup: row = %s" % row)
                if str(row) == "1":
                    success = True
        if not success:
            self.log("stop_backup_func(): Unable to stop base backup", "ERROR")
        return success

    # Simple function to help ensure we have all paths created for postgresql
    def dbinit_func(self):
        check = standby.check_pgpid_func()
        if check == 0:
            self.log("dbinit_func(): Can not execute --dbinit with PG running locally", "ERROR")
            return False
        # check whether pgdata and tablespace paths exist, create if not.
        try:
            paths = self.primary_get_tablespace_paths()
            paths.insert(0, self.pgdata)
            for row in paths:
                if self.debug:
                    print "DEBUG: " + row
                row = row.rstrip('\n')
                if not os.path.isdir(row):
                    os.makedirs(row, 0700)
        except OSError, e:
            self.log("dbinit_func(): %s " % e, "ERROR")
            self.log("You may have permission problems; make sure user %s can create directory %s" % (self.user, self.pgdata))
        except Exception, e:
            self.log("dbinit_func(): %s " % e, "ERROR")
        else:
            self.log("dbinit_func(): Standby is ready")
            return True
        self.log("dbinit_func(): failed", "ERROR")
        return False

    # Takes a base backup of master. This function is tricky because
    # there is a possibility of a non 0 exit status even when successful
    def base_backup_func(self):
        # first, copy tablespaces
        paths = self.primary_get_tablespace_paths()
        for row in paths:
            row = row.rstrip('\n')
            self.debuglog("base_backup_func(): row = %s" % row)
            retval = self.rsync_dir(row)
            if retval and retval != 23 and retval != 24: # 23, 24 incomplete or vanished source file.
                self.log("base_backup_func(): Couldn't rsync", "ERROR")
                raise RsyncFailure
        # finally, copy over pgdata
        # Before, doing the rsync, make sure we cleanup pg_xlog for streaming replication
        # XXX: does it makes sense at all, given that we don't copy pg_xlog from master?
        if self.use_streaming_replication:
            self.debuglog("base_backup_func(): Cleaning up master pg_xlog directory before rsync")
            os.system("rm -rf " + self.pgdata + "/pg_xlog/*")
        master_pgdata = self.primary_get_datadir_path().strip('\n')
        retval = self.rsync_dir(master_pgdata, self.pgdata)
        if retval and retval != 23 and retval != 24:
            self.log("base_backup_func(): Couldn't rsync", "ERROR")
            raise RsyncFailure

    def rsync_dir(self, dirfrom, dirto):
        cmd_args = list(self.copy_dirs)  # make a copy
        # last arg of copy_dirs is "user@master:", concat the remote dir
        cmd_args[-1] += dirfrom + "/"
        cmd_args += [dirto + "/"]
        if self.rsync_threads > 1:
            trsync_path = os.path.join(self.pitr_bin_path, "threaded_rsync.py")
            cmd_args = [trsync_path, "--num_threads", str(self.rsync_threads), " ".join(cmd_args)]
        self.debuglog("running rsync as: %s" % cmd_args)
        return subprocess.call(cmd_args)

    # Start postgresql
    def start_postgresql_func(self):
        pgctl = self.pgctl_base
        pgctl.insert(1, "start")
        retval = subprocess.call(pgctl)
        if retval:
            self.notify_external(log=True, warning=True, message="Unable to start PostgreSQL")
            return False
        # check whether PostgreSQL has already started. Sleep to give it time to write
        # the pid file.
        sleep(2)
        check = standby.check_pgpid_func()
        if check != 0:
            self.log("start_postgresql_func(): PostgreSQL refused to start", "ERROR")
            return False
        return True

    # Stop postgresql
    def stop_postgresql_func(self):
        pgctl = self.pgctl_base + ["-m", "fast"]
        pgctl.insert(1, "stop")
        retval = subprocess.call(pgctl)
        if retval:
            self.notify_external(critical=True, message="Unable to stop PostgreSQL")
            return False
        return True

    # Writes recovery.conf file to pgdata
    def write_recovery_func(self):
        try:
            file = open(self.recovery_conf, 'w')
            file.write('%s' % (self.recovery_string))
            file.close()
        except Exception, e:
            self.log("write_recovery_func(): Unable to write recovery file %s" % self.recovery_conf)
            self.notify_external(critical=True, message="Unable to write recovery file: %s" % (e,))
            raise

    # Copies in production slave configurations from storage location to production pgdata location
    def copy_conf(self, failover=False):
        # return if we opt to set command-line options to read files from
        # their original location.
        if self.no_copy_conf:
            return
        if failover:
            pg_conf = self.pg_conf_failover;
            pg_hba_conf = self.pg_hba_conf_failover;
        else:
            pg_conf = self.pg_conf;
            pg_hba_conf = self.pg_hba_conf;

        for f in (pg_conf, pg_hba_conf):
            try:
                if f:
                    if os.access(f, os.F_OK | os.R_OK):
                        shutil.copy(f, self.pgdata)
                    else:
                        self.log("copy_conf(): %s is inaccessible" % f, "WARNING")
            except Exception:
                self.log("Unable to copy configuration file %s" % f, "ERROR")
                raise

    # Standby function, we need to write the recovery configuration
    # and start postgresql
    def standby_func(self):
        result = False
        check = standby.check_pgpid_func()
        if check == 0:
            self.log("standby_func(): Can not enter standby mode if PG is already running", "ERROR")
            return False
        try:
            self.write_recovery_func()
        except Exception, e:
            self.log("standby_func(): %s" % e, "ERROR")
        else:
            self.copy_conf()
            result = self.start_postgresql_func()
        if not result:
            self.log("standby_func(): Unable to enter standby mode", "ERROR")
        else:
            self.log("standby_func(): Successfully entered standby mode")
        return result

    # Function allows you to specify a script to execute on failover
    # The script must return 0 to be considered successful
    def failover_action_func(self):
        if self.action_failover:
            retval = system("%s" % (self.action_failover))
            if retval:
                self.notify_external(critical=True, message="failover action returned non-zero exit code")
                return False
            else:
                self.log("Statistics are not replicated in warm standby mode. Try executing ANALYZE on your databases.")
                return True

    def run_base_backup(self):
        result = False
        check = self.check_pgpid_func()
        if check == 0:
            self.log("run_base_backup(): Cannot take base backup with PG running locally", "ERROR")
        else:
            try:
                self.start_backup_func()
                self.debuglog("start_backup_func() handled")
                self.base_backup_func()
                self.debuglog("base_backup_func() handled")
                if self.check_pgxlog_path_func():
                    result = True
                    self.debuglog("check_pgxlog_path_func() handled")
                else:
                    self.notify_external(critical=True, message="unable to access xlog path")
            except RsyncFailure:
                self.log("run_base_backup(): rsync returned non-zero exit status. Check logged errors as this may be harmless.", "WARNING")
            except RemoteNoData, e:
                self.log("run_base_backup(): remote host returned no data for query: %s" % e, "ERROR")
            except SSHFailure:
                self.log("run_base_backup(): received error code 255 from ssh, likely due to the wrong password", "ERROR")
            except Exception, e:
                self.log("run_base_backup(): %s" % e, "ERROR")
            finally:
                result = self.stop_backup_func() and result
        if result:
            self.log("run_base_backup(): Base backup finished successfully")
        else:
            self.log("run_base_backup(): Base backup is unsuccessful and should be restarted", "ERROR")
        return result

    def do_failover(self, options):
        success = True
        try:
            check = self.check_pgpid_func()
            # Note: we already check that use_streaming_replication is not issued
            # with recovertotime inside check_config_func.
            if self.use_streaming_replication:
                if check == 0:
                    # Postgres is running, just touch the trigger file and all should be good.
                    try:
                        file = open(self.trigger_file, "w")
                        file.close()
                    except:
                        self.log("do_failover(): unable to write trigger file %s" % self.trigger_file, "ERROR")
                        raise
                else:
                    # pgsql is NOT running, and we are trying to failover.  Lets try to rename recovery.conf into recovery.stopped and try to start pgsql.
                    try:
                        os.rename(self.recovery_conf, self.recovery_stopped_file)
                    except:
                        self.log("do_failover(): unable to rename %s to %s" % (self.recovery_conf, self.recovery_stopped_file), "ERROR")
                        raise
            else:
                # XXX: trying to stop the server that does recovery may not work
                if check == 0:
                    self.stop_postgresql_func()
                self.write_recovery_func()
            # XXX: if it's streaming replication and the server is already running - than it's running with an old set of configuration files.
            self.copy_conf(failover=True)
        except Exception, e:
            self.log("do_failover(): %s" % e, "ERROR")
            success = False
        else:
            # try to start PostgreSQL if it's not running
            if not self.use_streaming_replication or check != 0:
                success = success and self.start_postgresql_func()
            if not success:
                self.log("do_failover(): failover FAILED", "ERROR")
            else:
                self.log("do_failover(): failover finished")
            # technically, failover has succeeded at this moment, but we still may fail
            # at the failover action script. Distinguish that case from failure at failover
            success = success and self.failover_action_func()
            if success:
                self.notify_external(ok=True, message="successful failover")
        return success

if __name__ == '__main__':
    # before we do anything, let's just check you we are
    if os.geteuid() == 0:
        sys.exit("\nBad Mojo... no root access for this script\n")

    standby = CMDStandby(classdict)
    (options, args) = standby.parse_commandline_arguments(argslist, CMDStandby.command_line_check_func)

    configfilename = options.configfilename
    base_backup = options.base_backup
    setstandby = options.standby
    dbinit = options.dbinit
    pgctl_action = options.pgctl_action
    ping_check = options.ping_check
    failover = options.failover
    recovertotime = options.recovertotime

    try:
        cfg_vals = standby.load_configuration_file(configfilename)
        # Locate executables
        standby.get_bin_paths_func(options)
        # Before we do anything, let's check the configuration
        standby.check_config_func(options)
        # Locate executables on master
        standby.get_remote_psql()
        # Configure ssh command-line string, rsync options,
        # pg_standy arguments and recovery options.
        standby.set_options(options)
    except Exception, e:
        standby.log(e, "ERROR")
        sys.exit(2)

    success = False
    # perform different actions depending on command-line switches
    if dbinit:
        success = standby.dbinit_func()
    elif base_backup:
        success = standby.run_base_backup()
        # If we want to failover to the latest good transaction
    elif options.failover == '999':
        success = standby.do_failover(options)
        # If we want to enter standby mode
    elif setstandby:
        success = standby.standby_func()
        # If we want to check if we can talk to the master
    elif ping_check:
        success = standby.ping_check_func()
    # If we want to start or stop postgresql on the slave
    elif pgctl_action == 'start':
        standby.copy_conf()
        success = standby.start_postgresql_func()
    elif pgctl_action == 'stop':
        success = standby.stop_postgresql_func()
    elif pgctl_action == 'stop_basebackup':
        success = standby.stop_backup_func()
    else:
        print "Config OK"
        success = True

    if success:
        sys.exit(0)
    else:
        sys.exit(1)
