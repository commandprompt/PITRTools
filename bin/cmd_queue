#!/usr/bin/env python

""" LICENSE

Copyright Command Prompt, Inc.

Permission to use, copy, modify, and distribute this software and its
documentation for any purpose, without fee, and without a written agreement
is hereby granted, provided that the above copyright notice and this
paragraph and the following two paragraphs appear in all copies.

IN NO EVENT SHALL THE COMMAND PROMPT, INC. BE LIABLE TO ANY PARTY FOR
DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES, INCLUDING
LOST PROFITS, ARISING OUT OF THE USE OF THIS SOFTWARE AND ITS DOCUMENTATION,
EVEN IF THE COMMAND PROMPT, INC. HAS BEEN ADVISED OF THE POSSIBILITY OF
SUCH DAMAGE.

THE COMMAND PROMPT, INC. SPECIFICALLY DISCLAIMS ANY WARRANTIES,
INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND
FITNESS FOR A PARTICULAR PURPOSE. THE SOFTWARE PROVIDED HEREUNDER IS ON AN
"AS IS" BASIS, AND THE COMMAND PROMPT, INC. HAS NO OBLIGATIONS TO
PROVIDE MAINTENANCE, SUPPORT, UPDATES, ENHANCEMENTS, OR MODIFICATIONS.

"""

import os
import errno
import subprocess
from cmd_worker import CMDWorker

argslist = (("-C", "--config",
             dict(dest="configfilename",
                  action="store",
                  help="the name of the archiver config file",
                  metavar="FILE",
                  default='cmd_archiver.ini')),
            ("-d", "--daemon",
             dict(dest="daemon",
                  action="store_true",
                  help="background daemon mode")))

classdict = (('rsync_flags', 's', ""),
             ('slaves', 's', None),
             ('user', 's', None),
             ('r_archivedir', 's', None),
             ('l_archivedir', 's', None),
             ('ssh_timeout', 'i', None),
             ('notify_ok', 's', None),
             ('notify_warning', 's', None),
             ('notify_critical', 's', None),
             ('debug', 'b', False),
             ('pgdata', 's', None),
             ('rsync_version', 'i', None),
             ('includepath', 's', None),
             ('ssh_debug', 'b', False),
             ('queue_user', 's', None),
             ('max_queue_workers', 'i', None),
             ('queue_wait', 'i', None))

pid_file_name = "cmd_queue.pid"

class CMDQueue(CMDWorker):

    def check_config(self):
        super(CMDQueue, self).check_config()

        if self.max_queue_workers <= 0:
            raise Exception("The max_queue_workers setting should be greater than zero.")

        import pwd
        if os.geteuid() != pwd.getpwnam(self.queue_user).pw_uid:
            raise Exception("Only user '%s' is allowed to run cmd_queue according to the config file." % self.queue_user)

        self.pid_file = os.path.join(self.l_archivedir, pid_file_name)

        pathvars = [self.pgdata, self.l_archivedir]
        for slave in self.slaves_list:
            pathvars.append(os.path.join(self.l_archivedir, slave))
        self.check_paths(pathvars)

    def list_pending_slaves(self):
        hosts = []
        for slave in self.slaves_list:
            try:
                if os.listdir(os.path.join(self.l_archivedir, slave)):
                    hosts.append(slave)
                    self.debuglog("slave `%s' queue not empty" % slave)
            except OSError, e:
                self.log(e, "ERROR")
        return hosts

    def ship_logs_to_slave(self, slave):
        cmd = [self.rsync]
        cmd.append("--remove-sent-files" if self.rsync_version == 2 else "--remove-source-files")
        archivepath = os.path.join(self.l_archivedir, slave, "") # ensure trailing /
        cmd.extend(['-e', '%s %s' % (self.ssh, self.ssh_flags),
                    '-r', archivepath,
                    '%s@%s:%s/' % (self.user, slave, self.r_archivedir)])
        # extra flags should follow default ones to take effect
        cmd.extend(self.rsync_flags.split())
        if self.debug:
            cmd.append("-v")
        else:
            cmd.append("-q")
        self.debuglog("Shipping pending logs via: %s" % cmd)
        return subprocess.Popen(cmd)

    def wait_for_worker(self):
        pid, ret = os.wait()
        self.debuglog("a child rsync worker PID %d finished with status %d" % (pid, ret))
        child = self.workers.pop(pid)
        if ret != 0:
            self.notify_external(log=True, critical=True, message="rsync error %d when shipping to %s" % (ret, child.slave_name))

    def process_new_archive_files(self):
        """
        Hard-link any files found under local archive dir to the slave
        subdirs, removing the original link name.  Skip our .pid file.

        Occasionally, we might grab a rsync's temp file that it's going
        to rename to the final destination.  Fortunately, these temp
        file names are starting with a dot.  Let's ignore any "hidden"
        files anyway.
        """
        self.debuglog("checking for new archive files in %s" % self.l_archivedir)
        files = os.listdir(self.l_archivedir)
        for name in files:
            file = os.path.join(self.l_archivedir, name)
            # process regular files only, exclude our own .pid file and
            # any "hidden" files
            if not name.startswith('.') and name != pid_file_name and os.path.isfile(file):
                self.debuglog("Found new archive file: %s" % name)
                # count the number of links to the original name
                linked = 0
                for slave in self.slaves_list:
                    target = os.path.join(self.l_archivedir, slave, name)
                    try:
                        os.link(file, target)
                        linked += 1
                    except OSError, e:
                        if e.errno == errno.EEXIST:
                            linked += 1
                        else:
                            self.notify_external(log=True, critical=True, message=("Failed to link archive file: %s" % e))
                # Only unlink the original name when every of the
                # slaves has got a link.
                self.debuglog("linked to %d slave dirs out of %d" % (linked, len(self.slaves_list)))
                if linked == len(self.slaves_list):
                    os.unlink(file)

    def update_slaves(self):
        self.process_new_archive_files()

        slaves = self.list_pending_slaves()
        self.debuglog("list of slaves pending sync: %s" % repr(slaves))
        for slave in slaves:
            self.debuglog("going to ship logs to slave '%s'" % slave)
            # Check if we've already running the maximum allowed
            # number of worker rsync processes, if so wait for one of
            # them to finish.
            if len(self.workers) >= self.max_queue_workers:
                self.debuglog("we're already running %d child workers, waiting..." % len(self.workers))
                self.wait_for_worker()

            # there's at least one free worker slot, start a worker:
            try:
                child = self.ship_logs_to_slave(slave)
                self.debuglog("started child rsync worker PID %d" % child.pid)
                child.slave_name = slave
                self.workers[child.pid] = child
            except Exception, e:
                self.log(e, "ERROR")

        if len(self.workers) > 0:
            self.debuglog("waiting for any remaining workers to finish...")
            while len(self.workers) > 0:
                self.wait_for_worker()
            self.debuglog("all workers finished")

    def check_postmaster_alive(self):
        return os.path.exists(os.path.join(self.pgdata, "postmaster.pid"))

    def run(self):
        import time

        self.workers = dict()  # a mapping of PIDs to Popen objects
        while True:
            self.update_slaves()
            if not self.check_postmaster_alive():
                # this is not the loop condition to make sure we run
                # at least once
                self.log("postmaster isn't running anymore, exiting", "NOTICE")
                break
            self.debuglog("sleeping for %d seconds" % self.queue_wait)
            time.sleep(self.queue_wait)

    def check_pid_file(self):
        file = None
        try:
            self.debuglog("trying to open pid file: %s" % self.pid_file)
            file = open(self.pid_file)
            other_pid = int(file.readline())
            self.debuglog("checking PID %d" % other_pid)
            os.kill(other_pid, 0)
            return False
        except IOError, e:
            self.debuglog(repr(e))
            if e.errno != errno.ENOENT:
                self.log("Failed to read the PID file: %s", e)
                return False
        except ValueError, e:
            # assume no other instance running
            self.debuglog(repr(e))
        except OSError, e:
            self.debuglog(repr(e))
            if e.errno != errno.ESRCH:  # No such process
                return False
        except Exception, e:
            self.log(e, "ERROR")
        finally:
            if file:
                file.close()
        return True # this means no other pid is running

    def make_pid_file(self):
        file = None
        try:
            self.debuglog("writing pid file: %s" % self.pid_file)
            file = open(self.pid_file, "w")
            file.write("%s\n" % os.getpid())
        finally:
            if file:
                file.close()

    def remove_pid_file(self):
        if os.path.exists(self.pid_file):
            os.unlink(self.pid_file)

    def main(self):
        # before we do anything, let's just check who we are
        if os.geteuid() == 0:
            exit("\nBad Mojo... no root access for this script\n")

        retval = 0
        try:
            self.parse_commandline_arguments(argslist)
            self.load_configuration_file()

            if not self.check_pid_file():
                print "An instance of cmd_queue is already running?"
                # this should return success as cmd_archiver is
                # running us every time
            else:
                if self.options.daemon:
                    self.debuglog("going into background daemon mode...")
                    # set SIGHUP handler beforehand to avoid race
                    # condition after fork()
                    import signal
                    signal.signal(signal.SIGHUP, signal.SIG_IGN)

                    pid = os.fork()
                    if pid != 0:
                        self.debuglog("forked PID %d" % pid)
                        exit(0)
                    # the child process goes on

                try:
                    self.make_pid_file()
                    self.run()
                finally:
                    self.remove_pid_file()

        except Exception, e:
            self.log(e, "ERROR")
            retval = 1

        self.debuglog("cmd_queue exiting with status %d" % retval)
        return retval


if __name__ == '__main__':
    queue = CMDQueue(classdict)
    retval = queue.main()
    exit(retval)
